%************************************************
\chapter{Evaluation}
\label{chapter:evaulation}
%************************************************

\section{``Boot-up'' and Perceive Experiment}

\autoref{table:bootup} on page \pageref{table:bootup} shows the mind
during a \emph{``boot-up'' and perceive experiment}, an experiment
where the AI is created and simply continues to perceive each new
state of the physical world over time.  This experiment shows that the
mind takes $10$ minutes to boot up.  Then, it learns from its
perceptions over the next $50$ minutes.  Over the $50$ minutes of
continuous perception, the mind uses a highly fluctuating amount of
memory that stays, on average, relatively constant.  The mind is not
idle in this experiment.  The mind contains resources that allocate an
average of 1.8 megabytes per second in the process of detecting visual
changes in the next state from the physical simulation, over 6
gigabytes allocated over the entire 60 minutes of the experiment.  The
mind uses an average total memory of about 26 megabytes by the end of
the experiment.  The bytecode execution rate stabilizes at about 12
thousand bytecodes per second.  The architecture can handle bursts of
50 thousand bytecodes per second, but this experiment simply shows
that the execution rate of the architecture does not slow down over
time.  The entire system exhibits stable handling of continuous
processing for experiments lasting over an hour, handling the
allocation and garbage collection of many gigabytes of memory in the
process.

\section{Deliberative and Reflective Learning Experiments}


\section{No Theoretical Slowdown of Original Algorithm}

In order to assume that there is no theoretical slowdown of the
original planning algorithm when reflective heuristic learning is
applied, it must be assumed that the reflective implementation is an
ideal concurrent shared memory architecture.  In practice, the
underlying Funk virtual operating system slows down when more
concurrent and parallel fibers are executing.  This is beside the
theoretical point of this thesis, but the following table shows a
real-time test of the actual slowdown experienced by the Funk
operating system as different numbers of parallel fiber tasks are
executed to perform a simple numerical processing task.  The test was
done on a dual Pentium processor computer, each with ``core duo''
technology with each core implementing ``hyper-threading'', which ends
up appearing as eight processors to the Linux operating system
underlying the Funk virtual operating system:

\vspace{5mm}
\begin{tabular}{ll}
Tasks & Real-Time (s) \\
1 & 29\\
2 & 36\\
3 & 46\\
4 & 44\\
5 & 68\\
6 & 67\\
7 & 73\\
8 & 110\\
\end{tabular}
\vspace{5mm}

If a non-reflective learning algorithm is running on this hardware
implementation of a concurrent shared memory architecture and uses one
fiber and each reflective learning algorithm uses an additional fiber,
the Funk virtual operating system underlying SALS will experience
slowdown similar to that shown in this table.

\section{All Activities are Performed in Constant Time}

In order for all activities in my model to be reflectively optimized
by the $n$ layers of procedural reflective learning and planning, all
activities must be composed of primitive activities that are known to
complete in constant time, $O(1)$.  I have clearly described a
planning machine that operates based on constant time operations,
similar to a virtual machine.

In order to maintain this invariant in every aspect of the
implementation, I have needed to make some limiting assumptions in
order to avoid the general NP-complete problem of subgraph isomorphism
as a means of implementing symbolic perception.  If symbolic
perception is implemented as a subgraph isomorphism, then the
reflective optimization benefits of my model of reflective thinking no
longer apply.  In order to avoid this dangerous problem in my
implementation, I have made two temporary proof-of-concept simplifying
assumptions: (1) restricting learned symbolic perceptions to subgraphs
of 4 nodes and 4 edges or less of a constant upper-bound in
complexity, and (2) by hand-coding a small number of procedural
critics in the second-order reflective thinking layer that observe
specific properties of plans, such as similarly restricted subgraph
combinations of expected transframe additions and removals of plans in
different planning machine registers.

The general solution to the problem of symbolic perception in my model
is the planning-to-perceive problem as described by
\cite{pryorcollins:1995}.  In this way, the first-order planning
machine is responsible for creating plans that can be compiled into
perceptual resources that are composed of constant time, $O(1)$, steps
that respond to the perceptual event stream originating from the
layers below.  These plans must be composed of actions that traverse
and compare node and edge labels of the event stream as well as the
reflectively reconstructed representation of the layers below in order
to implement different planned ways to recognize the same symbol
depending on the order of the events in the stream.

It is imperative to not confuse the symbolic perception problem in my
model with the general NP-complete subgraph isomorphism problem.  In
this way, every aspect of my model, including symbolic perception,
recognizing isomorphic subgraphs in the layers below, becomes a
problem that can be optimized through procedural reflection, depending
on the order of reflective events.

\section{Efficiency Gains of Heuristic Learning}

\cite{mitchell:1997} defines the \emph{inductive learning hypothesis}
as follows:
\begin{definition}\emph{
\emph{The inductive learning hypothesis.} Any hypothesis found to
approximate the target function well over a sufficiently large set of
training examples will also approximate the target function well over
other unobserved examples.  }\end{definition} As the first-order
reflective learning algorithm learns hypotheses that generalize the
effects of actions from symbolic perceptions, the inductive learning
hypothesis is assumed of the target $\text{reflective}^0$ or physical
layer under the reflective focus.  In order to see efficiency gains in
the second-order reflective learning layer, I define here the
\emph{reflective inductive learning hypothesis}:
\begin{definition}\emph{
\emph{The reflective inductive learning hypothesis.} Any heuristic
found to approximate the target planning failure well over a
sufficiently large set of plan execution examples will also
approximate the target execution failure well over other unobserved
plan executions.}\end{definition} As long as the reflective inductive
learning hypothesis holds for a given planning domain, arbitrarily
greater planning efficiencies can be expected for each reflective
layer where this hypothesis holds true.

\section{Comparing Temporal and Reflective Learning}

A reflective learning algorithm implies arbitrarily better learning
algorithms, including one-shot learning algorithms.  In many domains,
the opportunity to learn is rare.  When the cost of failure is high,
it is important to learn as much as possible from each failure.

I will refer to machine learning algorithms that learn by assigning
credit for failures to the temporally previous action as
\emph{temporal learning} algorithms.  Contemporary temporal learning
algorithms are relatively advanced, some even having relational
object-oriented models of actions and the world.  Temporal learning
algorithms learn by assigning credit for a failure to the previous
time step or the previous action.  A reinforcement learning algorithm
that learns in this immediately temporal sense is called the
\emph{temporal difference learning} algorithm as described by
\cite*{kaelbling:1996}.  In temporal learning, the previous physical
action is considered in the context of the previous physical state of
the world.  Preconditions and postconditions for the action are
updated.  In other words, the categories of the world are updated for
this action, given the unexpected reward or punishment, a failure in
expectations.  In the temporal difference learning algorithm, there is
a focus on the previous physical action and physical state combination
in order to assign credit for failures.  Temporal learning algorithms
assign credit back, sequentially in time, from action to action.

Reflective learning algorithms can be thought of as having a temporal
learning algorithm within each necessarily distinct layer of
knowledge.  Reflective learning can take advantage of concurrent
processors for each separate layer of activity without slowing down
the primary learning algorithm under focus.  Let $n$ be the number of
reflective layers in a learning algorithm with each layer on a
concurrent processor.  If there is a memory of actions and the state
of the world for the previous time step, a temporal learning algorithm
spends $O(1)$ time relearning the effects of the temporally previous
action.  This process can be repeated for probabilistic algorithms,
resulting in a statistical spreading of credit through the uncertainty
of a number of previous actions.  The reflective algorithm also spends
$O(1)$ time but relearns the effects of $n$ actions, where $n$ is
limited by memory and processors.
{\mbox{\autoref{figure:learning_complexities}}} shows a comparison of
the time and space complexities of temporal and reflective learning
algorithms as well as the number of learning opportunities afforded by
one failure.
\begin{figure}
\center
\begin{tabular}{p{2cm}|p{2cm}|p{2cm}|p{3cm}}
Learning Algorithm & Time   & Space  & One-shot Learning Opportunities \\ \hline
Temporal           & $O(1)$ & $O(1)$ & $1$ \\
Reflective         & $O(1)$ & $O(n)$ & $n$ \\
\end{tabular}
\caption{Temporal and reflective learning complexities with one-shot learning opportunities.}
\label{figure:learning_complexities}
\end{figure}



\section{Space Complexity}

\section{Conclusion}



