%************************************************
\chapter{Evaluation}
\label{chapter:evaluation}
%************************************************

The AI consists of 86 resources, organized into 15 agencies, which
comprise the 4 layers.  The underlying \emph{Funk} virtual machine and
programming language \cite[]{morgan:2009} takes advantage of
concurrent hardware and simulates further parallelism through a 8
virtual bytecode machine processors.  Causal procedurally reflective
tracing allows for the run-time evaluation of every separate causally
scoped component of the architecture.  In order to evaluate the
cognitive architecture, three run-time evaluations have been
performed:
\begin{enumerate}
\item \emph{Boot-up and Perceive}: The mind initializes without goals
  and simply perceives the world over time.  This experiment functions
  as a control, showing that the mind is stable and does not use an
  increasing amount of memory or slow down as time passes.
\item \emph{No Reflective Learning}: The deliberative layer recalls a
  plan from memory, imagines its effects, executes the plan, the plan
  fails.  This experiment is used to compare how much additional time
  and memory is used when the reflective learning, imagination, and
  plan execution are enabled in the architecture.
\item \emph{Deliberative and Reflective Learning (The Full
  Architecture)}: It takes approximately 140 minutes to execute the
  full architecture through the entire reflective learning example,
  finally leading to successfully accomplishing the goal, after
  learning a new model of the physical world as well as a new model of
  the deliberative planning machine.
\end{enumerate}

In each experiment statistics are gathered for each resource, agency,
and the entire mind, i.e. memory allocation in bytes, garbage
collection in bytes, bytecode execution counts for each causally
scoped component.  During the experiments resources in the cognitive
architecture are free to start any number of parallel processes to
handle their processing and the causal procedurally reflective tracing
still accounts for these processes as part of the resource's causal
scope of execution.

\section{Boot-up and Perceive Experiment}

As an initial evaluation of the run-time performance of the AI, I have
run an experiment that simply initializes the AI and lets it perceive
the world over time.  The physical world does not change during this
experiment and the AI is not given any goals to accomplish, so this
experiment is a control that shows the baseline memory usage and
run-time performance of the cognitive architecture in its ``idling''
mode.  This experiment shows that the mind takes $10$ minutes to
initialize.  After initializing, the AI learns from its initial
perception and continues to attempt to detect changes in its raw
visual input over the next $50$ minutes.  Over the $50$ minutes of
continuous perception, the mind uses a highly fluctuating amount of
memory that stays, on average, relatively constant.  The mind is not
static in this experiment.  The mind contains resources that allocate
an average of $1.8$ megabytes per second in the built-in reactive
layer process of detecting visual changes in the next state from the
physical simulation, so that these changes may be propagated to the
physical knowledge of the learned reactive layer in a stream of
events.  Over $6$ gigabytes of memory is allocated over the entire
$60$ minutes of the experiment.  The mind uses an average memory
footprint of about $26$ megabytes over the course of the experiment.
The bytecode execution rate stabilizes at about $12$ thousand
bytecodes per second.  The concurrent processing capabilities of the
architecture can handle bursts of $50$ thousand bytecodes per second,
but this experiment simply shows that the execution rate of the
architecture does not slow down over time.  The entire cognitive
architecture exhibits stable handling of continuous processing for
experiments lasting over an hour, handling the allocation and garbage
collection of many gigabytes of memory in the process.
{\mbox{\autoref{figure:data/bootup_evaluation/mind_plot-Gripper-1}}}
shows a plotted overview of the run-time performance of the AI during
the boot-up and perceive experiment.  \autoref{table:bootup} on page
\pageref{table:bootup} shows more detailed plots for the run-time
behavior of each individual layer and agency within the AI.
\experimentAIdatafigures[60]{bootup_evaluation}{an experiment that
  boots up the AI, allowing it to continue to perceive the physical
  world over time without stimulating it to achieve any goals.  This
  experiment shows that the AI learns from its initial perceptions,
  and when these do not change, it has nothing to learn and memory
  usage is stable.}

\section{Deliberative Learning Experiment without Reflection}

The second run-time experiment that was performed with the cognitive
architecture does not include the reflective learning component that
learns hypothetical models for how the deliberative resources modify
the planning machine type knowledge base.  The AI is initialized,
which takes the same $10$ minutes as in the initial experiment.  The
AI is then told to execute a deliberative plan, which causes it to
execute a plan that attempts to stack a cube on top of a pyramid.  In
learning the effects of reactive resource executions on the physical
type knowledge bases the deliberative AI allocates $5.3$ megabytes per
second, while the memory footprint only grows at a rate of $90$
thousand bytes per second due to the learning that occurs over the
$60$ minutes that it takes to fail to accomplish this goal.  The AI
does not respond to the failure because the reflective layers are not
active.  The AI allocates an average of $1.2$ megabytes of memory per
second for the length of the experiment, which results in a total of
$5.4$ gigabytes over the course of the experiment.  The architecture
executes an average of $11$ thousand bytecodes per second over the
course of the experiment.  This shows that the architecture slows down
when more resources are active than simply the reactive perceptual
resources.  As deliberative learning resources become active, the
bytecode rate does not decrease much from the $12$ thousand bytecodes
per second in the control, but the overall memory allocation rate does
decrease by a factor of $66$\% from $1.8$ megabytes per second in the
control to $1.2$ megabytes per second with deliberative learning
enabled.
{\mbox{\autoref{figure:data/no_reflective_learning_evaluation/mind_plot-Gripper-1}}}
shows a plotted overview of the run-time performance of the AI during
the deliberative learning experiment with the reflective layer
disabled.  \autoref{table:no_reflective_learning} on page
\pageref{table:no_reflective_learning} shows plots of data for the
entire mind, each layer, as well as each agency within each layer for
this deliberative learning experiment.
\experimentAIdatafigures[75]{no_reflective_learning_evaluation}{an
  experiment with the reflective tracing of the deliberative process
  disabled.  This experiment can be compared with run-time behavior of
  the full AI cognitive architecture, which can learn from the failure
  and initiate a second attempt at plan selection and execution.}

\section{Deliberative and Reflective Learning Experiment (The Full Architecture)}

The third run-time experiment that was performed with the cognitive
architecture includes both the deliberative as well as reflective
learning layers.  The deliberative layer learns hypothetical models of
the reactive resource execution effects on the physical type knowledge
base, while the reflective layer learns hypothetical models of the
deliberative resource execution effects on the deliberative planning
machine type knowledge base.  Although this experiment involves an
initial imaginative and plan selection processes in the reflective
layer, this experiment runs very similarly through the first $75$
minutes of otherwise mostly deliberative and reactive resource
executions.  Around minute $75$, the cognitive architecture responds
to the failure in the deliberative planning machine with the
reflective bug response, which begins another round of reflective
learning, imagination, and refined plan selection.  After a reflective
bug response, the goal is accomplished successfully around minute
$105$.  The experiment is allowed to run continuously after this point
for a total of $180$ minutes in order to show the performance of
baseline perceptual activity after going through the complete process
of deliberative and reflective learning.  Except for a blip at minute
$135$, which requires an extended garbage collection period, the AI is
otherwise stable after both layers have successfully demonstrated
learning.  With the addition of the reflective learning layer in the
final experiment the average bytecode execution rate has dropped
significantly, running at a factor $55$\% of the execution rate of the
architecture in the experiment demonstrating only deliberative
learning.  This experiment demonstrates the taxing demand of the extra
concurrent processes and memory allocation required with the
additional reflective learning algorithm.  However, slowing the
algorithm down by a factor of two with the addition of a reflective
layer of learning shows that the slow down is roughly linear with the
additional layer.  For example, in a naive approach to reflective
learning, an exponential increase in processing would be expected
because all resource executions and knowledge in the layers below
would be reflected upon and modelled by the reflective layer.  Because
the reflective learner implemented in this architecture only reflects
over the deliberative planning machine and not all of the processing
in the layers below, this architecture shows a linear slowdown with
the addition of reflective layers of learning.
{\mbox{\autoref{figure:data/reflective_learning_evaluation/mind_plot-Gripper-1}}}
shows a plotted overview of the run-time performance of the AI during
the reflective learning experiment.
\autoref{table:reflective_learning} on page
\pageref{table:reflective_learning} shows plots of data for the entire
mind, each layer, as well as each agency within each layer for this
reflective learning experiment, showing the run-time performance of
the full architecture.
\experimentAIdatafigures[180]{reflective_learning_evaluation}{an
  experiment testing the full AI cognitive architecture, including the
  reflective tracing of the deliberative planning process, imagining
  the potential failures of the deliberative planning machine as well
  as the physical effects of physical actions.}

\section{No Theoretical Slowdown of Original Algorithm}

In order to assume that there is no theoretical slowdown of the
original planning algorithm when reflective learning is applied, it
must be assumed that the reflective implementation is an ideal
concurrent shared memory architecture.  In practice, the underlying
Funk virtual operating system slows down when more concurrent and
parallel tasks are executing.  This is beside the theoretical point,
but the following table shows a real-time test of the actual slowdown
experienced by the Funk operating system as different numbers of
parallel tasks are executed to perform a simple numerical processing
task.  The test was done on a dual Pentium processor computer, each
with ``core duo'' technology with each core implementing
``hyper-threading'', which ends up appearing as eight processors to
the Linux operating system underlying the Funk virtual operating
system:

\vspace{5mm}
\begin{tabular}{ll}
Tasks & Real-Time (s) \\
1 & 29\\
2 & 36\\
3 & 46\\
4 & 44\\
5 & 68\\
6 & 67\\
7 & 73\\
8 & 110\\
\end{tabular}
\vspace{5mm}

As each additional concurrent resource in the cognitive architecture
begins execution, this table shows the approximate slowdown that the
Funk virtual machine experiences on this specific dual processor
hardware.

\section{Efficiency Gains of Heuristic Learning}

\cite{mitchell:1997} defines the \emph{inductive learning hypothesis}
as follows:
\begin{definition}\emph{
\emph{The inductive learning hypothesis.} Any hypothesis found to
approximate the target function well over a sufficiently large set of
training examples will also approximate the target function well over
other unobserved examples.  }\end{definition} \noindent As the
deliberative learning algorithm learns hypotheses that generalize the
effects of reactive resources on the physical type knowledge base, the
inductive learning hypothesis is assumed of the target physical type
knowledge base and reactive resources under the deliberative learning
focus.  In order to see efficiency gains in the reflective learning
layer, which focuses on the deliberative planning resources' effects
on the deliberative planning machine type knowledge base, I define
here the \emph{reflective inductive learning hypothesis}:
\begin{definition}\emph{
\emph{The reflective inductive learning hypothesis.} Any hypothesis
found to approximate the target planning failure well over a
sufficiently large set of plan execution examples will also
approximate the target execution failure well over other unobserved
plan executions.}\end{definition} As long as the reflective inductive
learning hypothesis holds for a given planning domain, arbitrarily
greater planning efficiencies can be expected for each reflective
layer where this hypothesis holds true.

\section{Comparing Temporal and Reflective Learning}

A reflective learning algorithm implies arbitrarily better learning
algorithms, including one-shot learning algorithms.  In many domains,
the opportunity to learn is rare.  When the cost of failure is high,
it is important to learn as much as possible from each failure.

I will refer to machine learning algorithms that learn by assigning
credit for failures to the temporally previous action as
\emph{temporal learning} algorithms.  Contemporary temporal learning
algorithms are relatively advanced, some even having relational
object-oriented models of actions and the world.  Temporal learning
algorithms learn by assigning credit for a failure to the previous
time step or the previous action.  A reinforcement learning algorithm
that learns in this immediately temporal sense is called the
\emph{temporal difference learning} algorithm as described by
\cite*{kaelbling:1996}.  In temporal learning, the previous physical
action is considered in the context of the previous physical state of
the world.  Preconditions and postconditions for the action are
updated.  In other words, the categories of the world are updated for
this action, given the unexpected reward or punishment, a failure in
expectations.  In the temporal difference learning algorithm, there is
a focus on the previous physical action and physical state combination
in order to assign credit for failures.  Temporal learning algorithms
assign credit back, sequentially in time, from action to action.

Reflective learning algorithms can be thought of as having a temporal
learning algorithm within each necessarily distinct layer of
knowledge.  Reflective learning can take advantage of concurrent
processors for each separate layer of activity without
\emph{theoretically} slowing down the primary learning algorithm under
focus.  Let $n$ be the number of reflective layers in a learning
algorithm with each layer on a concurrent processor.  If there is a
memory of actions and the state of the world for the previous time
step, a temporal learning algorithm spends $O(1)$ time relearning the
effects of the temporally previous action.  This process can be
repeated for probabilistic algorithms, resulting in a statistical
spreading of credit through the uncertainty of a number of previous
actions.  The reflective algorithm also spends $O(1)$ time but
relearns the effects of $n$ actions, where $n$ is limited by memory
and processors.  {\mbox{\autoref{figure:learning_complexities}}} shows
a theoretical comparison of the time and space complexities of
temporal and reflective learning algorithms as well as the number of
learning opportunities afforded by one failure.
\begin{figure}
\center
\begin{tabular}{p{2cm}|p{2cm}|p{2cm}|p{3cm}}
Learning Algorithm & Time   & Space  & One-shot Learning Opportunities \\ \hline
Temporal           & $O(1)$ & $O(1)$ & $1$ \\
Reflective         & $O(1)$ & $O(n)$ & $n$ \\
\end{tabular}
\caption{Temporal and reflective learning complexities with one-shot learning opportunities.}
\label{figure:learning_complexities}
\end{figure}

