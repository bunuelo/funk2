%************************************************
\chapter{Grounded Learning of Knowledge Utility}
\label{chapter:grounded_learning_of_knowledge_utility}
%************************************************


Hypothetical physical type knowledge transframes are used by the
deliberative planning machine in order to infer the effects of
executing multi-step plans.  There is a distinction here made between
the knowledge that is directly an abstraction of knowledge grounded in
visual perception events, not based on any learned action hypotheses,
and the type of knowledge that is inferred based on hypotheses either
that it has ``been told'' or that the AI has learned in the course of
acting in the physical world.  In order to maintain this distinction
in the AI, the physical type knowledge that the AI infers is dependent
on hypotheses of resource execution effects and is stored in \emph{the
  counterfactual physical type knowledge base}.  Counterfactual
physical type knowledge is exactly the same as the previously
introduced physical type knowledge except for the key distinction that
it has three types of dependencies that can cause it to lose its
factual grounding:
\begin{enumerate}
\item Future resource activation dependencies.
\item Resource execution transframe change hypothesis dependencies.
\item Resource execution transframe precondition knowledge dependencies.
\end{enumerate}
If any of these three types of dependencies loses its hypothetical
factual grounding, this loss of hypothetical factual grounding is
propagated backwards through dependencies to the counterfactual
knowledge that it has supported.  In this way, factual perceptual
knowledge is abstracted into factual abstractions, which means more
factually grounded knowledge is in the visual, physical, and physical
type knowledge bases.  However, this addition of factually grounded
knowledge causes a loss of hypotheses from the resource execution
physical type knowledge transframe hypothesis spaces.  This loss of
hypotheses propagates to cause counterfactual knowledge that the AI
has inferred to no longer have hypothetical factual grounding.  As
some counterfactual knowledge forms the preconditions for further
counterfactual knowledge inference, this causes the loss of grounding
for counterfactual knowledge that was generated with correct
hypotheses but based on counterfactual preconditions that have lost
their factual grounding.  In this way, additional factual knowledge
refines and reduces the counterfactual knowledge that is considered to
be hypothetically factually grounded by the AI.
