%************************************************
\chapter{Learning Causal Hypotheses}
\label{chapter:learning_causal_hypotheses}
%************************************************

\section{Resources}

Activities in Duration can be referred to symbolically by the
reflective thinking layers.  While all activities in all layers are
activities in Duration, these activities can be symbolized as
potentially actionable parts of plans by the reflective thinking
layers.  I will refer to a symbolic reference to activities that can
be put into plans as a \emph{resource}.  Resources exist in the
thinking layers as symbolic references to activities in the layers
below.

\section{Activation and Suppression}

In my model, I've included a logical idea that I refer to as
\emph{suppression}.  Suppression is a symbolic relationship to a
resource that can be put into plans.  The idea of suppression is a
subtle point with respect to the activities in Duration.  The basic
problem is that the activities in Duration are the activities that
exist independently without thinking existing as any implicit,
co-existent necessity.  Further, the activities in Duration cannot be
inactive, by definition.  This is important.  In other words, a
symbolic reference to something inactive would imply a symbol that
refers to something that does not exist, or, in other words, only
exist as a contradiction.  Therefore, the logical idea of suppression
is only a static tool of thought, part of the thinking layers,
entirely separate from physical activities.  The subtle point here is
that suppression does not disable the potential for activities.
Suppression is only a logical Spatial relationship including the
symbolic reference to the physical activity in question.  I will refer
to the logical alternative to suppression as \emph{activation}.
Activation and suppression refer to types of assumed Spatial
relationships that are maintained between symbolic resources.
Therefore, it would not be correct to say that physical activities
have been activated or suppressed, but alternatively, it would be
correct to say that a resource is in an ongoing, unstoppable, dynamic
Spatial relationship that has activated or suppressed qualities.  The
activation and suppression of resources occurs in the sense of
actively creating Spatial relationships that represent the logical
qualities of activation or suppression, which have the potential to be
logically consistent or contradictory.

Therefore, in my model, a resource can be both either activated or
suppressed, which does not, by itself, imply a resultant activity in
the layer below; logically, if a resource is activated and is not
otherwise suppressed, then the resource is considered logically to be
activated; however, when a resource is both activated and suppressed
simultaneously, this is a logical failure that is cause for a plan to
halt execution.  Before discussing potential responses to plans
failing in this way, it will obviously be necessary to first discuss
the basics of the planning process.

\section{Cause and Effect}

Two symbols correlated in time are not enough to compose a causal
relationship.  Two symbols correlated in time are simply a coincident
transition from the past to the future.  A causal relationship
supposes an additional component, a necessary connective symbolic
reference to the activities that are ongoing during the transition.
Thus, the effect of the causal component becomes the transition
itself.  A causal relationship, therefore, has three parts: the
symbolized activities in the present, the symbolized activities in the
past, and the symbolized activities in the future.  I will sometimes
refer to these three parts of the causal relationship more succinctly
as (1) the cause, (2) the necessity, and (3) the result.  When causal
models are used for planning, the symbolic reference that is the cause
is referred to as a resource.

\section{Goal Activities}

It is important to understand that, in my model, goals are not
derivatives of symbolized perceptions.  Symbolized goals refer to the
fundamental activities in Duration that give \emph{a priori} direction
to the activities of thinking.  Goals can thus be arranged in Spatial
orders, according to preferential qualities; i.e. goals can be
considered to be positive or negative, something to seek or something
to avoid.

In my model, a goal is a reference to activities, similar to a
perception, except that it is arranged preferentially among other
goals to be sought or avoided.

The first-order reflective layer symbolizes goals from the physical
layer activities.  This creation of a symbolic reference to goal
activities becomes the reason for planning and acting toward or away
from the associated perceptions.  Note that because symbolic goals,
perceptions, and resources in my model are static and do not derive
from one another, a process of refining symbolic references to
perceptions and resources can be undertaken in the pursuit of creating
more accurate causal models that predict the activities in Duration
that symbolic goals reference.

\section{Causal Hypothesis}

When goal activities are symbolized, causal hypotheses can be created
for predicting the goal activities in the future.  For example, if
there are ongoing activities happening simultaneously with the
symbolization of the goal, these can be hypothesized as causes of a
future symbolized goal.  Remember that a causal model has three parts:
present cause, past necessity, and future result.  In this case, the
goal is placed in a future context, the symbolized current activity is
in the present, and there must also be a past symbol to fill the last
slot in the model.

\section{Limitations of Logical Goals}

Logical approaches to AI have seen the value in considering goals to
be relative arrangements of perceptual symbols.  These sorts of
symbolic relationships in my model are thought of as occurring
simultaneously with, before or after goal activities.  Static
constructions that are actively maintained in Spatial arrangement can
be reified as symbolic perceptions, but fundamentally the goal does
not refer to a static construction, despite the possible correlation
of goals with such constructions.  In this way, my model gains the
ability to think about and refine the symbolization of perceptions
because symbolic goals refer to the activities in Duration, instead of
the inversely limiting assumption that goals are specified in terms of
static constructions of perceptual symbols.  Static constructions are
useful tools for accomplishing goal activities; however, reflectively,
all static constructions, including symbols, are caused by the AI
itself and the correspondingly responsible activities can change.
Thus, all symbolic constructions are explicitly available as static
references that are available for inspection by the AI itself.  In
other words, in purely logical approaches to AI, symbols are not
understood to be static creations of the AI itself and, thus, cannot
be reflectively debugged when they are wrong.

In order to allow these logical approaches to adapt, I see no
alternative but to acknowledge the dynamic, allowing the potential for
debugging grounded symbolic constructions.  AI systems must use
symbols in full awareness of their creation as changeable static
references to a dynamic unstated actual existence.

\section{Physical Goals}

The first class of goal that I will discuss is called the
\emph{physical goal}.  Physical goals refer to dynamic activities in
Duration that are a cause for the creation and refinement of static
symbolic perceptions and resources in the reflective thinking layers.
These symbolic perceptions and resources are thus physical as well.

\section{Reflective Classes of Causal Models}

The first-order reflective layer creates causal models from physical
perceptions, resources, and goals.  I've described previously how
goals can cause reflective thinking to create causal hypotheses.
Another way to state this more succinctly is as follows: \emph{goals
  are the cause of causal hypotheses}.  Note two different meanings of
cause in the previous sentence.  In the latter case, I'm referring to
causal models relating physical perceptions and resources, and in the
former case, I'm referring to the first-order reflective activity that
causes the creation of the physical model.

Allowing activities of the first-order reflection to be available for
inspection by the AI itself allows my model to represent the
transitions caused by first-order reflective activities.  These
transitions are knowledge level changes.  By using this reflective
technique, a new class of causal model is created, categorically
different from the physical causal models that the first-order
reflective layer manipulates.  When the activities of the first-order
reflective layer are reflectively symbolized, my model can then be
motivated to learn to accomplish or avoid knowledge level goals, using
knowledge level causal hypotheses.

\section{Probabilistic Causal Models}

Probabilistic models are an advanced and important thinking tool.
Building a probabilistic causal model involves counting symbolic
perceptions, resources, and goals.  For example, let us consider that
two causal hypotheses have been created that reference the same
symbolic cause and the same symbolic perception in the past; let us
say that the only difference between these two hypotheses is that they
refer to two different symbolic goals in the future.  This situation
gives my model a reason to further distinguish symbolic
representations for perceptions and resources, introducing more
refined symbols for these perceptions and resources in order to lead
to causal models that are more useful for correctly predicting these
two different symbolic goals.  This would be the more appropriate
course of action if we wanted to correctly predict the symbolic goals;
however, there is an opportunity here to build a probabilistic model
that does not refine the symbolization of perceptions or resources.
For example, both of the two causal hypotheses could be considered
together to compose a probabilistic causal hypothesis.  Using this
probabilistic hypothesis, a future inference could be created that
includes both symbolic goals, each with one half of a potential
existence.  Probabilistic causal hypotheses are useful for predicting
the average number of times that a symbolic event will occur.  Note
that probabilistic causal hypotheses require counting and creating
ratios from the more fundamental non-probabilistic causal hypotheses
from which they are constructed.

