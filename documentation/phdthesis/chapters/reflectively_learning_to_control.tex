%************************************************
\chapter{Reflectively Learning to Control}
\label{chapter:reflectively_learning_to_control}
%************************************************

In order to explain my implemented solution to the problem of
reflectively learning-to-control, I will focus on a running example
within the physical block building domain shown in
\autoref{figure:an_example_problem_domain}.  Consider that the AI has
the goal of making a stack of two blocks.  The AI is now confronted
with the task of reasoning about how to get the physical world into a
state that matches this goal condition.  Specifically, the AI must
decide upon a plan of action, either creating a new plan from scratch,
or recalling a plan from memory that with some modifications seems
like it might work in this case.  When the AI considers the plans that
it knows, it remembers a plan that it has been told will result in a
stack of two blocks.  For example, the following plan assures that
there is a stack of two blocks as its final operation:
\begin{enumerate}
\item Move to the left until you are above a cube.
\item Reach and grab.
\item Assure that you are holding a cube.
\item Move to the right until you are above a pyramid.
\item Drop the block you are holding.
\item Assure that there is a cube on a pyramid.
\end{enumerate}
If the AI were to recall this plan and execute it in the situation
shown in \autoref{figure:an_example_problem_domain}, the AI would
encounter an expectation failure at the final assertion of the
expected post-conditions of this plan.
{\mbox{\autoref{figure:failure_to_stack_cube_on_pyramid}}} shows how
executing this plan in this situation might fail to result in a stack
of two blocks: the pyramidal block does not support the cubic block,
so the cube falls off of the pyramid and onto the table.  In the case
of a failure of expectations, a refined hypothetical model of the
physical world is learned, so that the same planning mistake will
never be made again.  In my AI, learning a new physical model of the
world amounts to relearning the hypotheses for the expected abstract
transframes that result from executing different physical actions in
different abstract physical preconditions.  In this case, the AI
learns many new refined hypotheses about the physical world.  For
example, the AI learns that executing the action of moving to the left
until a cube is below the gripper results in a cube being below the
gripper.  Of course, this may not generally be true, but the AI's
physical action hypotheses are refined in light of this added
experiential knowledge that predicts this change given the specific
preconditions.  The AI still retains many hypotheses that leave room
for the possibility that executing this action in other preconditions
may or may not result in this specific change.  For example, at this
point, the AI has no experience executing this action when there is
not a block to the left of the gripper.  In that case, the AI cannot
be sure that this action will result in a block being below the
gripper, and this uncertainty is represented in the hypothesis space
of the AI's concept learning algorithm.
\begin{figure}
\begin{center}
\includegraphics[width=8cm]{gfx/blocks_world_example-1}
\end{center}
\caption[An example problem domain.]{An example problem domain.}
\label{figure:an_example_problem_domain}
\end{figure}
\begin{figure}
\begin{center}
\begin{tabular}{p{4cm}p{4cm}}
1. \includegraphics[width=5cm]{gfx/blocks_world_example-1}  & 2. \includegraphics[width=5cm]{gfx/blocks_world_example-2} \\
3. \includegraphics[width=5cm]{gfx/blocks_world_example-3}  & 4. \includegraphics[width=5cm]{gfx/blocks_world_example-4} \\
5. \includegraphics[width=5cm]{gfx/blocks_world_example-5}  & 6. \includegraphics[width=5cm]{gfx/blocks_world_example-6} \\
7. \includegraphics[width=5cm]{gfx/blocks_world_example-7}  & 8. \includegraphics[width=5cm]{gfx/blocks_world_example-8}
\end{tabular}
\end{center}
\caption[A example of an expectation failure.]{A example of an
  expectation failure.  A plan is executed that asserts that a
  post-condition must be a stack of a cube on top of a triangle.}
\label{figure:failure_to_stack_cube_on_pyramid}
\end{figure}

In addition to learning a refined hypothetical model of the physical
world, the AI also learns a better model of how to control its
planning machine that chose this plan for some reason and decided to
execute it.  In other words, not only has the AI failed to act
physically but also the AI has failed in thinking about its plans.
The AI decided to execute a plan that ended up failing.  Because the
AI has a reflective learning algorithm, it has the additional ability
to learn a better model of the planning process itself---the planning
process that selected this plan, modified it, and decided to execute
it.

Deliberative reasoning processes are plans in the reflective
metacognitive reasoning layer.  The metacognitive planning machine
creates, reasons about, and executes plans for how to control the
first-level planning machine.  Thus, the AI has two layers of
planners: one planner that learns about physical processes and another
planner that learns about planning processes.  A plan for physical
action is manipulated by resources in the deliberative layer, while a
plan for deliberative action is manipulated by resources in the
reflective layer.

In this example, the plan resulted in an expectation failure.  The
plan attempted to assert that a cube is supported by a pyramid, which
was not true at the point when the assertion was executed in the
deliberative planning machine.  When a plan encounters a specific type
of failure, properties of the failure event are represented as a
failure object that is associated with the plan object that failed.
As seen by the reflective layer, the deliberative failure knowledge is
related to the plan in a way that is analogous to the way that the
deliberative layer sees a physical block being on top of another
physical block.  The deliberative layer learns how to control
relationships between physical objects, while the reflective layer
learns how to control relationships between deliberative objects, such
as the deliberative planning machine, plans, hypotheses, and execution
failures.

The reflective layer learns models of how deliberative actions change
the abstract knowledge relationships between objects in the
deliberative layer.  The reflective planning machine attempts to
accomplish abstract goals computed from this deliberative knowledge.
Here is a list of a few types of abstract knowledge that the
reflective layer can know about the objects in the deliberative layer:
\begin{itemize}
\item A plan that asserts that a cube is on a triangle is under the
  focus of the deliberative planning machine.
\item A plan that has had an expectation failure is being executed by
  the deliberative planning machine.
\item There is no plan currently executing in the deliberative
  planning machine.
\item There is a plan to drop a block that you are holding before
  reaching and grabbing.
\item The deliberative planning machine is focused on a plan to move
  to the left while concurrently moving to the right.
\end{itemize}
These abstract states are learned in the reflective layer by
processing a procedurally reflective event stream generated by the
deliberative layer planning processes.  These abstract states are
potential goals, preconditions, and effects in the reflective planning
machine that abstractly model how deliberative actions effect the
state of the deliberative layer.  Reflective hypotheses about the
deliberative layer are learned in terms of these abstract states.  The
reflective layer learns hypotheses for predicting abstract types of
plan execution failure knowledge from other abstract deliberative
knowledge preconditions.

In the example, executing the plan that the deliberative planner is
focused on is hypothesized to result in a plan having an expectation
failure event.  In this case, the reflectively planned execution of
the deliberative execution of the plan under deliberative focus
results in a stream of reflective event knowledge.  This stream of
reflective event knowledge is processed in parallel with the
deliberative planning machine operations.  The reflective layer
abstracts representations of the preconditions and effects of the
deliberative execution action.  In the example, this abstract
knowledge includes that the plan currently under focus contains an
assertion that a cube must be stacked on a pyramid.  The AI refines
many of its hypotheses given this limited experience, retaining the
hypothesis that executing the plan under focus might result in failure
due to the plan's assertion that a cube is on top of a pyramid.

\section{The Three Layers of the Implementation}

\section{The Physical Simulation}

The block building domain is implemented as a simulation based on
two-dimensional ridid-body physical laws, including floating point
numerical representations for object positions, velocities and
accellerations.  These numerical representations and the processes
that manipulate them are part of the physical layer of the model, but
in order to focus the efficiency of the procedural reflection, a much
simpler relational graph representation has been specifically
represented as semantic frame-based objects in a semantic
knowledge-base, referred to as the \emph{physical knowledge-base}.
For example, the semantic knowledge can be thought of as a list of the
following types of sentences:
\begin{itemize}
\item Block-1 is a block.
\item Block-2 is a block.
\item Table-1 is a table.
\item Gripper-1 is a gripper.
\item Block-1 has a blue color.
\item Block-1 has a cube shape.
\item Block-2 has a green color.
\item Block-2 has a pyramid shape.
\item Table-1 has a white color.
\item Gripper-1 has a black color.
\item Block-1 is on Table-1.
\item Block-2 is on Table-1.
\item Gripper-1 is above Table-1.
\item Block-1 is to the left of Gripper-1.
\item Block-2 is to the right of Gripper-1.
\end{itemize}
These sentences can be slightly rearranged to be thought of as
subject-verb-object triples as in the following list:
\begin{itemize}
\item Block-1 is-a block.
\item Block-2 is-a block.
\item Table-1 is-a table.
\item Gripper-1 is-a gripper.
\item Block-1 has-a-color blue.
\item Block-1 has-a-shape cube.
\item Block-2 has-a-color green.
\item Block-2 has-a-shape pyramid.
\item Table-1 has-a-color white.
\item Gripper-1 has-a-color black.
\item Block-1 is-on Table-1.
\item Block-2 is-on Table-1.
\item Gripper-1 is-above Table-1.
\item Block-1 is-to-the-left-of Gripper-1.
\item Block-2 is-to-the-right-of Gripper-1.
\end{itemize}
The physical simulation maintains a list of these simple
subject-verb-object sentences in its internal state.  The actual
internal lisp-like representation of the physical simulation is shown
in \autoref{table:internal_lisp_like_physical_representation}.
\begin{table}
\begin{center}
{\fbox{
\begin{tabular}{l}
 {\tt{[Block-1 left-of Gripper-1]}} \\
 {\tt{[Block-1 on Table-1]}} \\
 {\tt{[Block-1 shape cube]}} \\
 {\tt{[Block-1 color blue]}} \\
 {\tt{[Block-1 is-a block]}} \\
 {\tt{[Block-2 right-of Gripper-1]}} \\
 {\tt{[Block-2 on Table-1]}} \\
 {\tt{[Block-2 shape pyramid]}} \\
 {\tt{[Block-2 color green]}} \\
 {\tt{[Block-2 is-a block]}} \\
 {\tt{[Table-1 left-of Gripper-1]}} \\
 {\tt{[Table-1 shape cube]}} \\
 {\tt{[Table-1 color white]}} \\
 {\tt{[Table-1 is-a table]}} \\
 {\tt{[Gripper-1 is-holding []]}} \\
 {\tt{[Gripper-1 color black]}} \\
 {\tt{[Gripper-1 is-a gripper]}} \\
 {\tt{[Gripper-1 movement\_command stop]}} \\
 {\tt{[Gripper-1 is me]}}
\end{tabular}
}}
\end{center}
\caption[Internal representation for physical simulation.]{Internal representation for physical simulation.}
\label{table:internal_lisp_like_physical_representation}
\end{table}
{\mbox{\autoref{figure:implemented_physical_knowledge}}} shows the
physical knowledge-base that is reconstructed from these reflective
change events.
\begin{sidewaysfigure}
\begin{center}
\includegraphics[width=24cm]{gfx/implemented_physical_knowledge}
\end{center}
\hspace{4cm}\parbox{15cm}{\caption[The physical knowledge-base.]{The
    physical
    knowledge-base.}\label{figure:implemented_physical_knowledge}}
\end{sidewaysfigure}

\section{Procedurally Reflective Event Stream}



\section{Abstraction}



\section{The Result of Reflective Learning}

{\mbox{\autoref{figure:implemented_example_learning_storyboard}}}
shows a storyboard of the implemented example of second-order
learning.
\begin{figure}
\begin{center}
\begin{tabular}{p{4cm}p{4cm}p{4cm}}
1. \includegraphics[width=4cm]{gfx/blocks_world_example-1}  & 2. \includegraphics[width=4cm]{gfx/blocks_world_example-2}  & 3. \includegraphics[width=4cm]{gfx/blocks_world_example-3} \\
4. \includegraphics[width=4cm]{gfx/blocks_world_example-4}  & 5. \includegraphics[width=4cm]{gfx/blocks_world_example-5}  & 6. \includegraphics[width=4cm]{gfx/blocks_world_example-6} \\
7. \includegraphics[width=4cm]{gfx/blocks_world_example-7}  & 8. \includegraphics[width=4cm]{gfx/blocks_world_example-8}  & 9. \includegraphics[width=4cm]{gfx/blocks_world_example-9} \\
10. \includegraphics[width=4cm]{gfx/blocks_world_example-10} & 11. \includegraphics[width=4cm]{gfx/blocks_world_example-11} & 12. \includegraphics[width=4cm]{gfx/blocks_world_example-12}
\end{tabular}
\end{center}
\caption[A storyboard of the implemented example of second-order
  learning.]{A storyboarded example of second-order learning, where
  the goal is to create a stack of two blocks.  (1) A plan that is
  hypothesized to stack the square block on top of the triangular
  block is executed.  (2--8) The plan completes execution and results
  in the square falling off of the triangle, an expectation failure.
  (9) The second-order hypothetical heuristic is learned that predicts
  plan failure will result by executing plans that try to stack
  squares on top of triangles.  A plan that is hypothesized to stack
  the triangle on top of the square is executed.  (9-12) The second
  plan completes execution, accomplishing the physical goal.}
\label{figure:implemented_example_learning_storyboard}
\end{figure}

