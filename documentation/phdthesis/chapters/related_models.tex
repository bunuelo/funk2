%************************************************
\chapter{Related Models}
\label{chapter:related_models}
%************************************************

\section{The Emotion Machine}
\label{backreference:self_reflective_self_conscious}

\cite{minsky:2006} describes a six-layer reflective model of mind,
\emph{{\mbox{Model-6}}: The Emotion Machine}.  Minsky's model reflects
over a \emph{physical simulation} that provides an interface to an
external world with sensors and actuators for controlling a physical
body.  {\mbox{Model-6}} has two reactive layers, built-in and learned,
that I have found useful for organizing my implementation.  Although
these layers are not key, I have found them good for organizing the
interface between an objective physical simulation and the lower
boundary of the first-order reflective layer of my model.  The
combination of the physical simulation and both reactive layers of
{\mbox{Model-6}} map to activities in the pre-symbolic physical or
$\text{reflective}^0$ layer of my model.  The ``deliberative'' layer
of Minsky's model maps to the first-order reflective thinking layer in
my model, but this is only very approximate because Minsky's
deliberative layer is not limited to reasoning about causal models of
the physical simulation as is this layer in my model.  In both of our
models, this layer makes plans using causal models that include
references to physical activities.  Minsky's ``reflective'' layer maps
to the union of all of the reflective layers in my model that are
equal to or greater than order two.  In both models, this layer
responds to failures in planning or plan execution, calling upon
higher orders of causal models to guide a debugging response.

\begin{figure}[bth]
\begin{align*}
\left.
  \begin{array}{l}
    \text{Minsky's Physical Simulation}\\
    \text{Minsky's Built-in Reactive Layer}\\
    \text{Minsky's Learned Reactive Layer}\\
  \end{array}
\right\}                            &{\approx} \text{ reflective}^0 \\
\text{Minsky's Deliberative Layer } &{\approx} \text{ reflective}^1 \\
\text{Minsky's Reflective Layer }   &{\approx} \bigcup_{n=2}^{\infty}{\text{reflective}^n}
\end{align*}
\caption{The lower layers of Model-6 mapped to my
  $\text{reflective}^n$ order notation.}
\label{figure:model_6_as_reflective_order_notation}
\end{figure}

\autoref{figure:model_6_as_reflective_order_notation} shows how
Minsky's layers map to my $\text{reflective}^n$ order notation.
Despite my use of set theoretic mathematical notation, I do not mean
to imply that the activities that can be thought of as layers are
actually sets of anything specific, especially not symbols, since the
physical layer refers to activities, but these are not symbolic or
discrete or separate or even static by definition.  I think the set
theoretic notation is a useful tool for thinking as long as we don't
require it to imply everything included in the mathematical
definitions, specifically, the derivation of sets by the potentially
infinite enumeration of discrete elements, which would obviously be
absurd in the present context.  I use the set notation only to show a
picture of how to think about the mapping between the models and not
to subsequently formally prove anything about the relationship between
our models by using the rules of mathematical set theory.

Above Minsky's reflective layer are the ``self-reflective'' and
``self-conscious'' layers of reflective thinking.  I have not
implemented these layers because they are not simple extensions upward
in my model.  I describe how I see these layers being implemented in
\autoref{section:model_6_future_research} as future research.

\section{Metareasoning}

\cite{cox_and_raja:2008} present a reflective model of mind that they
refer to as \emph{metareasoning}.  The metareasoning model they
present begins with a ``ground level'', which corresponds to my
physical, $\text{reflective}^0$, layer.  They then describe how a
second level, the ``object level'', is a control loop that receives
perceptions from the ground level, processes these, and sends
actionable commands back to the ground level.  Their object level
corresponds with my first-order reflective layer.  They then proceed
to a third level, which completes two cascaded control loops: one
controlling the ground level with another controlling the object
level.  This third level is called the ``meta-level''.  Cox and Raja's
meta-level corresponds to my second-order reflective layer, but Cox
and Raja's model might be interpretted to limit the meta-level to
thinking about the object level, which is not true of my
$\text{reflective}^2$ layer in the sense that this layer can symbolize
any of the activities in the layers below it.
\begin{figure}[bth]
\begin{align*}
\text{Cox and Raja's Ground Level } &{\approx} \text{ reflective}^0 \\
\text{Cox and Raja's Object Level } &{\approx} \text{ reflective}^1 \\
\text{Cox and Raja's Meta-Level }   &{\approx} \text{ reflective}^2 \\
\text{Cox and Raja's $\text{Meta}^2$-Level } &{\approx} \text{ reflective}^3 \\
\text{Cox and Raja's $\text{Meta}^n$-Level } &{\approx} \text{ reflective}^{(n+1)} \\
\end{align*}
\caption{The lower levels of Cox and Raja's metareasoning model mapped
  to my $\text{reflective}^n$ order notation.}
\label{figure:metareasoning_as_reflective_order_notation}
\end{figure}
\autoref{figure:metareasoning_as_reflective_order_notation} shows how
Cox and Raja's levels map to my $\text{reflective}^n$ order notation.
Here they explain what happens when the object level fails in either
the creation or execution of a plan:
\begin{quote}
When reasoning fails at some task, it may involve the explanation of
the causal contributions of failure and the diagnosis of the object-
level reasoning process.
\end{quote}
Cox and Raja explain how debugging causal models of the reasoning
process itself is a form of meta-level reasoning, or second-order
reflective thinking.  My model has a separate distinct class of causal
model for each layer of reflective thinking, so learning physical
causal models is a distinct activity from learning first-order
reflective thinking causal models.  My implementation demonstrates how
two distinct layers of causal models allow not only learning to act
but also learning to think about acting.

\section{Interior Grounding}

\cite{minsky:2005} describes an evolutionary reflective model of mind
that he calls \emph{interior grounding}.  Interior grounding states
that each layer of reflective thinking could be genetically
predestined to each have different and specific types of useful ways
of thinking.  Minsky rejects the \emph{physical grounding hypothesis},
which stipulates that thoughts must necessarily develop from the
lowest layer first and only subsequently to the higher layers of
thinking.

In terms of my model, activities in the mind can create and manipulate
symbolic arrangements without these symbols necessarily referring to
the physical layer of activity.  My model does not conflict with
modelling different developmental stages of minds or genetic
differences between minds.  My implementation begins the simulation in
a specific physical state, where the reflective layers of the mind are
already all actively thinking with explicit sets of pre-existing
knowledge, such as plans.  My model learns new knowledge and debugs
the knowledge it already has as the simulation proceeds.

