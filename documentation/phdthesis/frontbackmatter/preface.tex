%\manualmark
%\markboth{\spacedlowsmallcaps{Preface}}{\spacedlowsmallcaps{Preface}} % work-around to have small caps also
\refstepcounter{dummy}

%************************************************
\addtocontents{toc}{\protect\vspace{\beforebibskip}} % to have the bib a bit from the rest in the toc
\addcontentsline{toc}{chapter}{\tocEntry{Preface}}
%************************************************

\chapter*{Preface}
\chaptermark{Preface}

The story of how I have gotten to this point along my current
trajectory of AI research is mostly idiosyncratic and not especially
interesting.  However, only in this story is there reason for my
current position.  As a young student, I grew up like most, learning
that the fundamental basis of my world is a real number domain of
space and time.  When I spread my arms I thought of real distances and
when I remembered the past and projected my future I thought of my
life as embedded in a real number line.  My first learning algorithms
were artificial neural networks, which because of their basis in real
numbers were close to my understanding of my physical self and the
physical world.  I used derivatives and integrals to predict events
within space and time.  I followed my intuitions from these numerical
instincts, which lead me to make probabilistic Gaussian mixture models
describing distributions of commonsense narratives over space and
time.  It was difficult to describe complete logics over these
relational numerical distribution spaces.  This model was an inference
model---nothing more.  My goal was to understand processes of
deliberation and reflective thinking over these narrative spaces, and
this inference model was going to be the basis of my next project.
What does a process look like in this narrative space?  I would need a
more complete logic in order to specify transition functions, in order
to simulate more general processes that could be deliberative.  I saw
my inference system, although complicated, as a reactive system
without much actual thought going on.  At this point, I read through
many of Marvin Minsky's old papers and discussed my project with him.
He was very skeptical that I would make much progress at all if I had
probabilistic representations as the fundamental representation in my
model.  Then, my goal was clearly before me.  At this point, I reread
almost all of Minsky's papers and his student's PhDs, including Gerry
Sussman's work on HACKER, a system that reflectively debugs its own
planning system.  I came to an epiphany at this point.  I saw Minsky's
Society of Mind and Emotion Machine as a detailed diagram for an
intricate new human programming language.  Every single part of the
history of AI suddenly fell into place given this overarching goal.
The representation for a process that I had been so confused about
with my work on probabilistic models suddenly slapped me in the face.
I had been staring at it for over twenty years.  The representation
for a process is the programming language itself.  As I approached
this new realization cautiously, I became more comfortable in my
conclusion through conversations with my close friend and theoretical
physicist, Julian Avila.  Through these discussions, I realized that
although much of physical science had turned to a fundamental
understanding of the world as probabilistic, I didn't agree.  A
probabilistic model must be based on something discrete that is
counted, in order to be rational.  Now, my task was clear, if I was
going to build something like a probabilistic model that counted, I
would first need something to count.  I abandoned my previous work
that treated probability as fundamental and worked on understanding
and trying to build Minsky's model, which I saw as a new programming
language that ``closes the loop'' not only once but twice---once for
perceiving and acting, and again for reflectively perceiving and
debugging the program itself.  In this PhD, I describe the resulting
learning system that does just this: closes the loop twice.

